{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dylanoco/basketball-classification-dribbling/blob/main/creating_ts_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCtwsGyB6Y5b"
      },
      "source": [
        "## The Process of Creating a Dataset\n",
        "\n",
        "I will be using dummy data I created by moving my arm around for twelve seconds.\n",
        "This will be used to help me convert the CSV file recieved into a compatable .ts file type to be used for training.\n",
        "Once I have done this for the Accelerometer, Gyroscope and then finally both of them combined, I will proceed to create the actual training and testing dataset for the models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc4d09vyFjbq",
        "outputId": "747d14a2-f762-483c-f356-7d2f56e88a87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCIvOV-DCRaU"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAv4MSIo1GYJ"
      },
      "outputs": [],
      "source": [
        "# Converting CSV into a DataFrame for manipulation\n",
        "df_accel_data = pd.read_csv('/content/drive/MyDrive/Development_Project/dummy_data/Accelerometer.csv')\n",
        "#df_gyro_data = pd.read_csv('/content/drive/MyDrive/Development_Project/dummy_data/Gyroscope.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Resampling the Seconds\n",
        "df_accel_data['seconds_elapsed'] = pd.to_datetime(df_accel_data['seconds_elapsed'], unit='s')\n",
        "\n",
        "df_accel_data.set_index('seconds_elapsed', inplace=True)\n",
        "\n",
        "df_resampled = df_accel_data.resample('S').mean()\n",
        "\n",
        "df_resampled.reset_index(inplace=True)\n",
        "\n",
        "df_resampled['seconds_elapsed'] = df_resampled['seconds_elapsed'].astype(int) / 1e6\n",
        "\n",
        "# Print the resampled DataFrame\n",
        "print(df_resampled)\n"
      ],
      "metadata": {
        "id": "h1oNnZsyPlxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dropping Columns\n",
        "\n",
        "Now, I must drop the necessary columns so that I only remain with; z,y and x."
      ],
      "metadata": {
        "id": "1s6Pg_FPIcT9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_accel_data = df_accel_data.drop(columns=['time','seconds_elapsed'])"
      ],
      "metadata": {
        "id": "XKu6dNeFcZAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_resampled = df_resampled.drop(columns=['time','seconds_elapsed'])"
      ],
      "metadata": {
        "id": "xetiX3pSP_kW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0FoZZENBq_A"
      },
      "source": [
        "## Normalisation; MinMax()\n",
        "\n",
        "I will apply Normalisation to this data. This will help with consistency in the future and aid in combatting biases the model may develop during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5PbTUUdSB-y6"
      },
      "outputs": [],
      "source": [
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Selecting the axes, scaling them with MinMaxScaler\n",
        "df_resampled[['z', 'y', 'x']] = scaler.fit_transform(df_resampled[['z', 'y', 'x']])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1lWDqfMEoFm"
      },
      "source": [
        "## Establishing the labels (Y)\n",
        "\n",
        "Of course, each instance needs a label for the Classification models to be able to identify the movement performed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3hBXHWJSE0K-"
      },
      "outputs": [],
      "source": [
        "label = 'test_1'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmqflxEmE4Ja"
      },
      "source": [
        "## Exporting the data\n",
        "\n",
        "Finally, we want to be able to export this data into a suitable format. I have decided to use the .ts format from SKTime.\n",
        "\n",
        "A Time-Series file requires data in the format, specified in the documentation;\n",
        "\"The dataset in a 3d ndarray to be written as a ts file which must be of the structure specified in the documentation examples/loading_data.ipynb. (n_instances, n_columns, n_timepoints)\"\n",
        "\n",
        "As a result, I need to transpose the data so that the rows and columns are essentially flipped; 3 rows, n columns (n being the amount of timepoints, the 'rows' in this case are the columns z,y and x.)\n",
        "\n",
        "Then, once inserted into the array, the shape will match the format required\n",
        "\n",
        "*   (1 instance, 3 rows, n columns)\n",
        "*   \"(n_instances, n_columns, n_timepoints)\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqzsLXdkIbDL"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade sktime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sktime.datasets import write_ndarray_to_tsfile"
      ],
      "metadata": {
        "id": "g3wQzJYXyW_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_resampled = np.transpose(df_resampled)"
      ],
      "metadata": {
        "id": "k50N8gyXQK0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6RjmsDBFBkd"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "X = []\n",
        "y = []\n",
        "X.append(df_resampled)\n",
        "y.append(label)\n",
        "X = np.asarray(X)\n",
        "y = np.asarray(y)\n",
        "sy = set(y)\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Development_Project/'\n",
        "write_ndarray_to_tsfile(data = X, path = file_path, problem_name='sample_data', class_label=sy,class_value_list=y)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Allowing for more than one instance.\n",
        "\n",
        "Now that we can successfully create one instance of a movement, insert it into a dataset and create that dataset, we now need to make it capable of iterating through numerous instances.\n",
        "\n",
        "This will consist of creating a loop, iterating through a folder of CSV files, preprocessing each one of them and appending it into the list.\n",
        "\n",
        "Once completed, we then convert that list into a numpy Array to then be packed away as a Time-Series dataset !"
      ],
      "metadata": {
        "id": "eEvG3v2WS5FQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random"
      ],
      "metadata": {
        "id": "AQFMmJDyV_nt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = []\n",
        "y = []"
      ],
      "metadata": {
        "id": "CPNpjP0FW74J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "path = '/content/drive/MyDrive/Development_Project/dummy_data/'\n",
        "files = os.listdir(path)\n",
        "for file in files:\n",
        "\n",
        "  df_accel_data = pd.read_csv(path+file)\n",
        "  df_accel_data['seconds_elapsed'] = pd.to_datetime(df_accel_data['seconds_elapsed'], unit='s')\n",
        "  df_accel_data.set_index('seconds_elapsed', inplace=True)\n",
        "  df_resampled = df_accel_data.resample('S').mean()\n",
        "  df_resampled.reset_index(inplace=True)\n",
        "  df_resampled['seconds_elapsed'] = df_resampled['seconds_elapsed'].astype(int) / 1e6\n",
        "\n",
        "  df_resampled = df_resampled.drop(columns=['time','seconds_elapsed'])\n",
        "\n",
        "  scaler = MinMaxScaler()\n",
        "  # Selecting the axes, scaling them with MinMaxScaler\n",
        "  df_resampled[['z', 'y', 'x']] = scaler.fit_transform(df_resampled[['z', 'y', 'x']])\n",
        "\n",
        "  df_resampled = np.transpose(df_resampled)\n",
        "\n",
        "  X.append(df_resampled)\n",
        "  y.append(label + str(random.randint(0,5)))\n",
        "\n",
        "X = np.asarray(X)\n",
        "y = np.asarray(y)\n",
        "sy = set(y)\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Development_Project/'\n",
        "write_ndarray_to_tsfile(data = X, path = file_path, problem_name='sample_data', class_label=sy,class_value_list=y)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "U7pFZz41TdlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## End Result\n",
        "\n",
        "We now have an ndarray with a shape of (4,3,13); 4 Instances (I had 4 CSV Files in one folder), 3 Columns (X,Y, and Z) and 13 Timepoints (Recorded at 12 seconds but the 0th second is included).\n",
        "\n",
        "We can now record as many instances as we want, insert it into a folder of our choice and generate a time-series dataset.\n",
        "## Part 2\n",
        "However, not only are we going to record individual datasets (one for accelerometer and another for gyroscope), we also need to combine the two.\n",
        "\n",
        "As a result, we need to;\n",
        "* Be able to iterate through folders\n",
        "* differentiate between Accelerometer and Gyroscope csv files\n",
        "* rename the columns and combine the two dataframes together to count as one instance!\n",
        "\n"
      ],
      "metadata": {
        "id": "0kp_AijKaiA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "dir_path = '/content/drive/MyDrive/Development_Project/dummy_data/'\n",
        "files = os.listdir(path)\n",
        "\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for root,files,dir in os.walk(dir_path): #This allows us to walk through the folder that has more folders(instances)\n",
        "  for name in files: #Gets the name of the folders\n",
        "    path = os.path.join(dir_path,name)\n",
        "    folder_files = os.listdir(path)\n",
        "    count = 0\n",
        "    df_list = []\n",
        "    for file in folder_files: #Grabs the individual names of the files within that folder\n",
        "      file_name = file.split(\".\") #Allows us to split the name of the file from the file type extension\n",
        "      #Standard Preprocessing from earlier\n",
        "      df_accel_data = pd.read_csv(os.path.join(path,file))\n",
        "      df_accel_data['seconds_elapsed'] = pd.to_datetime(df_accel_data['seconds_elapsed'], unit='s')\n",
        "      df_accel_data.set_index('seconds_elapsed', inplace=True)\n",
        "      df_resampled = df_accel_data.resample('S').mean()\n",
        "      df_resampled.reset_index(inplace=True)\n",
        "      df_resampled['seconds_elapsed'] = df_resampled['seconds_elapsed'].astype(int) / 1e6\n",
        "      df_resampled = df_resampled.drop(columns=['time','seconds_elapsed'])\n",
        "      #By using .split(), we can get the name of the file and rename the columns as neccessary.\n",
        "      if(file_name[0] == \"Accelerometer\"):\n",
        "        df_resampled.rename(columns={\"z\":\"accel_z\", \"y\": \"accel_y\", \"x\": \"accel_x\"},inplace=True)\n",
        "        print(df_resampled.columns)\n",
        "        scaler = MinMaxScaler()\n",
        "        # Selecting the RENAMED axes\n",
        "        df_resampled[['accel_z', 'accel_y', 'accel_x']] = scaler.fit_transform(df_resampled[['accel_z', 'accel_y', 'accel_x']])\n",
        "      else: #Same process for Gyroscope\n",
        "        df_resampled.rename(columns={\"z\": \"gyro_z\", \"y\": \"gyro_y\", \"x\": \"gyro_x\"},inplace=True)\n",
        "        print(df_resampled.columns)\n",
        "        scaler = MinMaxScaler()\n",
        "        df_resampled[['gyro_z', 'gyro_y', 'gyro_x']] = scaler.fit_transform(df_resampled[['gyro_z', 'gyro_y', 'gyro_x']])\n",
        "\n",
        "      df_list.append(df_resampled)\n",
        "      count += 1\n",
        "\n",
        "    df_total = pd.concat(df_list, axis=1, join='inner')\n",
        "    df_total = np.transpose(df_total)\n",
        "    X.append(df_total)\n",
        "    y.append(label + str(random.randint(0,10)))\n",
        "\n",
        "X = np.asarray(X)\n",
        "y = np.asarray(y)\n",
        "sy = set(y)\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Development_Project/'\n",
        "write_ndarray_to_tsfile(data = X, path = file_path, problem_name='sample_data_combined', class_label=sy,class_value_list=y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXsP0EepsG_j",
        "outputId": "d291568f-1e62-4ccf-f2ae-655b324c7ddc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['accel_z', 'accel_y', 'accel_x'], dtype='object')\n",
            "Index(['gyro_z', 'gyro_y', 'gyro_x'], dtype='object')\n",
            "Index(['accel_z', 'accel_y', 'accel_x'], dtype='object')\n",
            "Index(['gyro_z', 'gyro_y', 'gyro_x'], dtype='object')\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "P1lWDqfMEoFm"
      ],
      "history_visible": true,
      "mount_file_id": "1oglg7EJlqlXwGF1dmEQUmY1JJY3aZ6-y",
      "authorship_tag": "ABX9TyPu74qFp7CFjWRxIJ4nYtxo",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}